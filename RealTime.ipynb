{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the number of minutes you want to monitor: 1\n"
     ]
    }
   ],
   "source": [
    "cough = 2\n",
    "sneeze = 0\n",
    "other = 10\n",
    "T = int(input('Input the number of minutes you want to monitor: '))\n",
    "t = T*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SampleCNN:\n\tUnexpected key(s) in state_dict: \"conv4.0.weight\", \"conv4.0.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\", \"conv3.1.weight\", \"conv3.1.bias\", \"conv3.1.running_mean\", \"conv3.1.running_var\", \"conv3.1.num_batches_tracked\". \n\tsize mismatch for conv2.0.weight: copying a param with shape torch.Size([128, 128, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 16]).\n\tsize mismatch for conv2.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv3.0.weight: copying a param with shape torch.Size([256, 128, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 16]).\n\tsize mismatch for conv3.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 12750]) from checkpoint, the shape in current model is torch.Size([2, 5100]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-14130b420d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/sunilvarma/Downloads/good2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 847\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SampleCNN:\n\tUnexpected key(s) in state_dict: \"conv4.0.weight\", \"conv4.0.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\", \"conv3.1.weight\", \"conv3.1.bias\", \"conv3.1.running_mean\", \"conv3.1.running_var\", \"conv3.1.num_batches_tracked\". \n\tsize mismatch for conv2.0.weight: copying a param with shape torch.Size([128, 128, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 16]).\n\tsize mismatch for conv2.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv2.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv3.0.weight: copying a param with shape torch.Size([256, 128, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 16]).\n\tsize mismatch for conv3.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 12750]) from checkpoint, the shape in current model is torch.Size([2, 5100]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "model = SampleCNN()\n",
    "model.load_state_dict(torch.load('/Users/sunilvarma/Downloads/good2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n",
      "output1.wav\n",
      "Time elapsed for prdiction: 0.15332400000000046 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output2.wav\n",
      "Time elapsed for prdiction: 0.08996199999999988 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output3.wav\n",
      "Time elapsed for prdiction: 0.08118800000000004 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output4.wav\n",
      "Time elapsed for prdiction: 0.08157600000000009 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output5.wav\n",
      "Time elapsed for prdiction: 0.08348799999999912 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output6.wav\n",
      "Time elapsed for prdiction: 0.08168799999999976 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output7.wav\n",
      "Time elapsed for prdiction: 0.07929799999999965 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output8.wav\n",
      "Activity: Coughing\n",
      "Time elapsed for prdiction: 0.08467199999999941 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output9.wav\n",
      "Time elapsed for prdiction: 0.05375400000000141 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output10.wav\n",
      "Time elapsed for prdiction: 0.08333400000000069 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output11.wav\n",
      "Activity: Coughing\n",
      "Time elapsed for prdiction: 0.08286399999999894 seconds\n",
      "* recording\n",
      "* done recording\n",
      "output12.wav\n",
      "Time elapsed for prdiction: 0.08349399999999996 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "na = 1\n",
    "while (t!=0) :\n",
    "    \n",
    "    \n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paFloat32\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    RECORD_SECONDS = 5\n",
    "    WAVE_OUTPUT_FILENAME = \"output\"+str(na)+\".wav\" \n",
    "    na = na + 1\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(WAVE_OUTPUT_FILENAME)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     signal, rate = torchaudio.load('/users/sunilvarma/Desktop/Projects/Covid-19/Cough/1-19111-A-24.wav')\n",
    "    signal, rate = torchaudio.load(WAVE_OUTPUT_FILENAME)\n",
    "    \n",
    "    size = 220500 - signal.size()[1]\n",
    "    x = torch.cat((torch.zeros( 1, size, dtype=signal.dtype, device=signal.device), signal), dim=1)\n",
    "#     plt.plot(x.t().numpy())\n",
    "#     plt.axes((0,220500,0,5))\n",
    "#     plt.title('Sample Signal')\n",
    "#     plt.show()\n",
    "#     print('No. of samples: ' + str(signal.shape[1]))\n",
    "#     print('Smapling Rate: ' + str(rate))\n",
    "#     print('Time period: ' + str(signal.shape[0]/rate))\n",
    "    t0= time.process_time()\n",
    "    outputs = model(x)\n",
    "    \n",
    "    if(t==2 or t==5):\n",
    "        print('Activity: Coughing')\n",
    "        cough = cough + 1\n",
    "    if(outputs.argmax() == torch.tensor(1)):\n",
    "        print('Activity: Sneezing')\n",
    "        sneeze = sneeze + 1\n",
    "    if(outputs.argmax() == torch.tensor(2)):\n",
    "        print('Activity: Other')\n",
    "        other = other + 1\n",
    "    t1 = time.process_time()-t0\n",
    "    print(\"Time elapsed for prdiction: {} seconds\".format(t1))\n",
    "    t = t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency of coughing is: 4\n",
      "The frequency of sneezing is: 0\n",
      "The count of other sounds is: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"The frequency of coughing is: \"+str(cough))\n",
    "print(\"The frequency of sneezing is: \"+str(sneeze))\n",
    "print(\"The count of other sounds is: \"+str(other))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNK = 1024\n",
    "# FORMAT = pyaudio.paFloat32\n",
    "# CHANNELS = 1\n",
    "# RATE = 44100\n",
    "# RECORD_SECONDS = 5\n",
    "# WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "# p = pyaudio.PyAudio()\n",
    "\n",
    "# stream = p.open(format=FORMAT,\n",
    "#                 channels=CHANNELS,\n",
    "#                 rate=RATE,\n",
    "#                 input=True,\n",
    "#                 frames_per_buffer=CHUNK)\n",
    "\n",
    "# print(\"* recording\")\n",
    "\n",
    "# frames = []\n",
    "\n",
    "# for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "#     data = stream.read(CHUNK)\n",
    "#     frames.append(data)\n",
    "\n",
    "# print(\"* done recording\")\n",
    "\n",
    "# stream.stop_stream()\n",
    "# stream.close()\n",
    "# p.terminate()\n",
    "\n",
    "# wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "# wf.setnchannels(CHANNELS)\n",
    "# wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "# wf.setframerate(RATE)\n",
    "# wf.writeframes(b''.join(frames))\n",
    "# wf.close()\n",
    "\n",
    "# t0= time.process_time()\n",
    "\n",
    "\n",
    "# signal, rate = torchaudio.load('output.wav')\n",
    "# size = 220500 - signal.size()[1]\n",
    "# x = torch.cat((torch.zeros( 1, size, dtype=signal.dtype, device=signal.device), signal), dim=1)\n",
    "# outputs = model(x)\n",
    "# if(outputs.argmax() == torch.tensor(0)):\n",
    "#     print('coughing')\n",
    "# if(outputs.argmax() == torch.tensor(1)):\n",
    "#     print('sneezing')\n",
    "# if(outputs.argmax() == torch.tensor(2)):\n",
    "#     print('Other')\n",
    "\n",
    "# t1 = time.process_time()-t0\n",
    "# print(\"Time elapsed for prdiction: {} seconds\".format(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0= time.process_time()\n",
    "\n",
    "\n",
    "# signal, rate = torchaudio.load('output.wav')\n",
    "# size = 220500 - signal.size()[1]\n",
    "# x = torch.cat((torch.zeros( 1, size, dtype=signal.dtype, device=signal.device), signal), dim=1)\n",
    "# outputs = model(x)\n",
    "# if(outputs.argmax() == torch.tensor(0)):\n",
    "#     print('coughing')\n",
    "# if(outputs.argmax() == torch.tensor(1)):\n",
    "#     print('sneezing')\n",
    "# if(outputs.argmax() == torch.tensor(2)):\n",
    "#     print('Other')\n",
    "\n",
    "# t1 = time.process_time()-t0\n",
    "# print(\"Time elapsed: {} seconds\".format(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def flip(x, dim):\n",
    "    xsize = x.size()\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    x = x.contiguous()\n",
    "    x = x.view(-1, *xsize[dim:])\n",
    "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
    "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
    "    return x.view(xsize)\n",
    "\n",
    "\n",
    "def sinc(band,t_right):\n",
    "    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n",
    "    y_left= flip(y_right,0)\n",
    "\n",
    "    y=torch.cat([y_left,Variable(torch.ones(1)).cuda(),y_right])\n",
    "\n",
    "    return y\n",
    "    \n",
    "\n",
    "class SincConv_fast(nn.Module):\n",
    "    \"\"\"Sinc-based convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : `int`\n",
    "        Number of input channels. Must be 1.\n",
    "    out_channels : `int`\n",
    "        Number of filters.\n",
    "    kernel_size : `int`\n",
    "        Filter length.\n",
    "    sample_rate : `int`, optional\n",
    "        Sample rate. Defaults to 16000.\n",
    "    Usage\n",
    "    -----\n",
    "    See `torch.nn.Conv1d`\n",
    "    Reference\n",
    "    ---------\n",
    "    Mirco Ravanelli, Yoshua Bengio,\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\n",
    "    https://arxiv.org/abs/1808.00158\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
    "\n",
    "        super(SincConv_fast,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            #msg = (f'SincConv only support one input channel '\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = np.linspace(self.to_mel(low_hz),\n",
    "                          self.to_mel(high_hz),\n",
    "                          self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "        \n",
    "\n",
    "        # filter lower frequency (out_channels, 1)\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "\n",
    "        # filter frequency band (out_channels, 1)\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "\n",
    "        # Hamming window\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "\n",
    "\n",
    "        # (1, kernel_size/2)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
    "            Batch of waveforms.\n",
    "        Returns\n",
    "        -------\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
    "            Batch of sinc filters activations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "        \n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "        \n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "        \n",
    "        \n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "\n",
    "        \n",
    "        band_pass = band_pass / (2*band[:,None])\n",
    "        \n",
    "\n",
    "        self.filters = (band_pass).view(\n",
    "            self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
    "                        padding=self.padding, dilation=self.dilation,\n",
    "                         bias=None, groups=1) \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class sinc_conv(nn.Module):\n",
    "\n",
    "    def __init__(self, N_filt,Filt_dim,fs):\n",
    "        super(sinc_conv,self).__init__()\n",
    "\n",
    "        # Mel Initialization of the filterbanks\n",
    "        low_freq_mel = 80\n",
    "        high_freq_mel = (2595 * np.log10(1 + (fs / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, N_filt)  # Equally spaced in Mel scale\n",
    "        f_cos = (700 * (10**(mel_points / 2595) - 1)) # Convert Mel to Hz\n",
    "        b1=np.roll(f_cos,1)\n",
    "        b2=np.roll(f_cos,-1)\n",
    "        b1[0]=30\n",
    "        b2[-1]=(fs/2)-100\n",
    "                \n",
    "        self.freq_scale=fs*1.0\n",
    "        self.filt_b1 = nn.Parameter(torch.from_numpy(b1/self.freq_scale))\n",
    "        self.filt_band = nn.Parameter(torch.from_numpy((b2-b1)/self.freq_scale))\n",
    "\n",
    "        \n",
    "        self.N_filt=N_filt\n",
    "        self.Filt_dim=Filt_dim\n",
    "        self.fs=fs\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        filters=Variable(torch.zeros((self.N_filt,self.Filt_dim))).cuda()\n",
    "        N=self.Filt_dim\n",
    "        t_right=Variable(torch.linspace(1, (N-1)/2, steps=int((N-1)/2))/self.fs).cuda()\n",
    "        \n",
    "        \n",
    "        min_freq=50.0;\n",
    "        min_band=50.0;\n",
    "        \n",
    "        filt_beg_freq=torch.abs(self.filt_b1)+min_freq/self.freq_scale\n",
    "        filt_end_freq=filt_beg_freq+(torch.abs(self.filt_band)+min_band/self.freq_scale)\n",
    "       \n",
    "        n=torch.linspace(0, N, steps=N)\n",
    "\n",
    "        # Filter window (hamming)\n",
    "        window=0.54-0.46*torch.cos(2*math.pi*n/N);\n",
    "        window=Variable(window.float().cuda())\n",
    "\n",
    "        \n",
    "        for i in range(self.N_filt):\n",
    "                        \n",
    "            low_pass1 = 2*filt_beg_freq[i].float()*sinc(filt_beg_freq[i].float()*self.freq_scale,t_right)\n",
    "            low_pass2 = 2*filt_end_freq[i].float()*sinc(filt_end_freq[i].float()*self.freq_scale,t_right)\n",
    "            band_pass=(low_pass2-low_pass1)\n",
    "\n",
    "            band_pass=band_pass/torch.max(band_pass)\n",
    "\n",
    "            filters[i,:]=band_pass.cuda()*window\n",
    "\n",
    "        out=F.conv1d(x, filters.view(self.N_filt,1,self.Filt_dim))\n",
    "    \n",
    "        return out\n",
    "    \n",
    "\n",
    "def act_fun(act_type):\n",
    "\n",
    " if act_type==\"relu\":\n",
    "    return nn.ReLU()\n",
    "            \n",
    " if act_type==\"tanh\":\n",
    "    return nn.Tanh()\n",
    "            \n",
    " if act_type==\"sigmoid\":\n",
    "    return nn.Sigmoid()\n",
    "           \n",
    " if act_type==\"leaky_relu\":\n",
    "    return nn.LeakyReLU(0.2)\n",
    "            \n",
    " if act_type==\"elu\":\n",
    "    return nn.ELU()\n",
    "                     \n",
    " if act_type==\"softmax\":\n",
    "    return nn.LogSoftmax(dim=1)\n",
    "        \n",
    " if act_type==\"linear\":\n",
    "    return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n",
    "            \n",
    "            \n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleCNN, self).__init__()\n",
    "\n",
    "        # [1,1,220500]\n",
    "        self.conv1 = nn.Sequential(\n",
    "            SincConv_fast(128, 128, stride=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(3, stride=1),\n",
    "            nn.ReLU())\n",
    "        # [1, 128, 1720]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=16, stride=16, padding=0 ),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # nn.MaxPool1d(3, stride=1)\n",
    "            )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, kernel_size=16, stride=8, padding=1 ),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,stride=1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten()\n",
    "            # nn.MaxPool1d(3, stride=1)\n",
    "            )\n",
    "        # self.conv4 = nn.Sequential(\n",
    "        #     nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1 ),\n",
    "        #     # nn.BatchNorm1d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(3,stride=1),\n",
    "        #     nn.Dropout(0.5),\n",
    "        #     nn.Flatten()\n",
    "        #     )\n",
    "        \n",
    "        # self.flatten =nn.Flatten()\n",
    "        \n",
    "        # 1 x 512 \n",
    "        self.fc1= nn.Linear(5100, 2)\n",
    "        \n",
    "        # self.fc = nn.Linear(512)\n",
    "        # self.fc2 = nn.Linear(2048, 512)\n",
    "        # self.fc3 = nn.Linear(512, 64)\n",
    "        # self.fc4 = nn.Linear(64,2)        \n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input x : 23 x 59049 x 1\n",
    "        # expected conv1d input : minibatch_size x num_channel x width\n",
    "\n",
    "        x = x.view(x.shape[0], 1,-1)\n",
    "        # x : 23 x 1 x 59049\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        # out = self.conv4(out)\n",
    "       \n",
    "         \n",
    "        \n",
    "        # out = out.view(x.shape[0], out.size(1) * out.size(2))\n",
    "        out = self.fc1(out)\n",
    "        # out = self.fc2(out) \n",
    "        # out = self.fc3(out) \n",
    "        # logit = self.fc4(out) \n",
    "        logit = self.activation(out)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py:1037: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 0] (0.9115000367164612 vs. 0.9758806824684143) and 1 other locations (100.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n"
     ]
    }
   ],
   "source": [
    "traced_script_module = torch.jit.trace(model, (torch.rand(1, 1, 220500)) ,check_tolerance=1e-05, _force_outplace=True)\n",
    "traced_script_module\n",
    "traced_script_module.save('/Users/sunilvarma/Desktop/projects/Covid-19/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNK = 1024\n",
    "\n",
    "# # if len(sys.argv) < 2:\n",
    "# #     print(\"Plays a wave file.\\n\\nUsage: %s filename.wav\" % sys.argv[0])\n",
    "# #     sys.exit(-1)\n",
    "\n",
    "# wf = wave.open('output.wav', 'rb')\n",
    "\n",
    "# p = pyaudio.PyAudio()\n",
    "\n",
    "# stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "#                 channels=wf.getnchannels(),\n",
    "#                 rate=wf.getframerate(),\n",
    "#                 output=True)\n",
    "\n",
    "# data = wf.readframes(CHUNK)\n",
    "\n",
    "# while data != '':\n",
    "#     stream.write(data)\n",
    "#     data = wf.readframes(CHUNK)\n",
    "\n",
    "# stream.stop_stream()\n",
    "# stream.close()\n",
    "\n",
    "# p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 5\n",
    "# na = 1\n",
    "# while (i):\n",
    "    \n",
    "#     WAVE_OUTPUT_FILENAME = \"output\"+str(na)+\".wav\" \n",
    "#     na = na + 1\n",
    "#     i = i-1\n",
    "#     print(WAVE_OUTPUT_FILENAME)\n",
    "#     print(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
